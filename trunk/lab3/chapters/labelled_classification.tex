\label{sec:labelled}
The MCID classifier was developed for each of the feature matrices for $n=2$, $n=8$, and $n=32$.  The provided feature matrices, \textbf{f2}, \textbf{f8}, and \textbf{f32} were used as learning data sets to teach the MCID classifiers the location and shape of the clusters.  This trained MCID classifier was then applied to the test data sets \textbf{f2t}, \textbf{f8t}, and \textbf{f32t}.  The performance of the classifier was assessed for each quantity of features using a confusion matrix.  The total probability of error is also shown for each case, shown in Figure \ref{tab:conf}.
\begin{table}[h]
\centering
\caption{Summary of Error analysis for $n=2, 8, 32$ with confusion matrices $M_{confusion}$, the probability of error for each image $P(\varepsilon|i)$, and the total probabiliy of error $P(\varepsilon)$ for each feature matrix}
\label{tab:conf}
\vspace{6pt}
\begin{tabular}{lcccc}
\toprule
 %& \multicolumn{1}{c}{Confusion Matrices and Probability of Error} \\
%\cmidrule(r){2-3} \cmidrule(r){4-5}
 & $M_{confusion}$ & $P(\varepsilon|i)$ & $P(\varepsilon)$ \\
\midrule
$2x2$ Feature Blocks &
\begin{bmatrix}
     1 &    0 &    0 &    2 &    3 &    0 &    1 &    4 &    5 &    0 &
     0 &    7 &    4 &    0 &    2 &    1 &    2 &    0 &    0 &    0 &
     0 &    3 &    0 &    3 &    0 &    0 &    1 &    1 &    7 &    1 &
     0 &    3 &    0 &    1 &    4 &    0 &    1 &    1 &    6 &    0 &
     1 &    0 &    0 &    2 &    4 &    0 &    0 &    5 &    4 &    0 &
     0 &    2 &    3 &    2 &    0 &    2 &    1 &    2 &    3 &    1 &
     0 &    5 &    4 &    0 &    2 &    0 &    1 &    0 &    4 &    0 &
     0 &    0 &    0 &    2 &    0 &    0 &    0 &    9 &    3 &    2 &
     2 &    1 &    2 &    4 &    2 &    0 &    2 &    1 &    2 &    0 &
     0 &    0 &    0 &    0 &    1 &    0 &    0 &    1 &    0 &   14 &
\end{bmatrix} &
\begin{bmatrix}
   0.9375\\
   0.5625\\
   1.0000\\
   0.9375\\
   0.7500\\
   0.8750\\
   0.9375\\
   0.4375\\
   0.8750\\
   0.1250\\
\end{bmatrix}
&  0.7438 & 
\\\addlinespace
$8x8$ Feature Blocks &
\begin{bmatrix}
     9 &    0 &    0 &    3 &    0 &    0 &    0 &    4 &    0 &    0 &
     0 &   10 &    2 &    0 &    0 &    1 &    3 &    0 &    0 &    0 &
     0 &    1 &    4 &    2 &    0 &    4 &    3 &    0 &    2 &    0 &
     1 &    0 &    0 &   12 &    0 &    1 &    1 &    0 &    1 &    0 &
     0 &    0 &    0 &    1 &    5 &    0 &    0 &    1 &    9 &    0 &
     0 &    0 &    4 &    3 &    0 &    2 &    4 &    0 &    3 &    0 &
     0 &    0 &    0 &    2 &    0 &    8 &    6 &    0 &    0 &    0 &
     0 &    0 &    0 &    0 &    2 &    0 &    0 &   10 &    0 &    4 &
     0 &    0 &    1 &    1 &    3 &    0 &    0 &    0 &   11 &    0 &
     0 &    0 &    0 &    0 &    1 &    0 &    0 &    0 &    2 &   13 &
\end{bmatrix} &
\begin{bmatrix}
   0.4375\\
   0.3750\\
   0.7500\\
   0.2500\\
   0.6875\\
   0.8750\\
   0.6250\\
   0.3750\\
   0.3125\\
   0.1875\\
\end{bmatrix}
&  0.4875 \\\addlinespace
$32x32$ Feature Blocks &
\begin{bmatrix}
	12 &    0 &    0 &    0 &    0 &    0 &    0 &    0 &    0 &    4 &
     0 &   16 &    0 &    0 &    0 &    0 &    0 &    0 &    0 &    0 &
     0 &    1 &   15 &    0 &    0 &    0 &    0 &    0 &    0 &    0 &
     0 &    0 &    0 &   16 &    0 &    0 &    0 &    0 &    0 &    0 &
     0 &    0 &    0 &    0 &   15 &    0 &    0 &    0 &    1 &    0 &
     0 &    0 &    6 &    1 &    0 &    7 &    2 &    0 &    0 &    0 &
     0 &    0 &    0 &    0 &    0 &    1 &   15 &    0 &    0 &    0 &
     0 &    0 &    0 &    0 &    0 &    0 &    0 &   11 &    0 &    5 &
     0 &    0 &    0 &    0 &    1 &    0 &    0 &    0 &   15 &    0 &
     0 &    0 &    0 &    0 &    0 &    0 &    0 &    1 &    0 &   15 &
\end{bmatrix} &
\begin{bmatrix}
    0.2500\\
    0.0000\\
    0.0625\\
    0.0000\\
    0.0625\\
    0.5625\\
    0.0625\\
    0.3125\\
    0.0625\\
    0.0625\\
\end{bmatrix}
& 0.1438 \\	\addlinespace
\bottomrule
\end{tabular}
\end{table}
\]
As we can see from the table on page \pageref{tab:conf}, the probability of a
misclassification drastically increases as we decrease our block size.  With
many features included, the resulting classifier performance is perfectly
acceptable and quite comparable with results obtained in previous labs. 
However, as we reduce the block size down to only $2\times2$, the resulting
classifier performance is abysmal. First, consider the data set based on
$32\times32$ feature blocks.  The classifier performs quite well on this data
set.  The confusion matrix shows relatively few misclassifications, The large
majority of the classifications appearing on the diagonal of the confusion
matrix, signifying a majority of correct classifications.  This observation is
perhaps best summarized by considering the associated probability of error. 
The overall probability of having a classification error is about $14\%$ with
this feature matrix.  This is in the same approximate range as the data sets
classified using MCID in previous labs, which is reassuring.  For even more
resolution on the performance of the MCID classifier on this test data set, the
probability of error for each individual image can be considered.  There are a
couple of instances, cotton and pigskin, in which no misclassifications are
made whatsoever.  There is one image that this classifier really struggles
with, and that is cork, with roughly a $50\%$ error rate.  Overall, we should be quite satisfied with the classifiers performance when 32x32 feature blocks are used. We will next jump to the case in which only 2x2 feature blocks are used to develop the MCID classifier.  This choice results in terrible classification performance.  Looking at the corresponding confusion matrix, we no longer see a heavy concentration along the diagonal.  Instead, the matrix is more evenly distributed, signifying much fewer correct classifications.  Over all ten images, the MCID classifier gets the clasification wrong $74.4%$ of the time using 2x2 feature blocks.  This means that we should actually actually expect the classification to be wrong more often than its correct.  Clearly this classifier will not be of great use due to its staggering lack of reliability.  In the specific example of the image of grass, the classifier does not manage to correctly classify even one data point.  Overall, we should be very dissatisfied with the performance of the classifier in this case. The 8 feature case provides a middle ground between the poor performance of the 2x2 feature blocks and the 32x32 feature blocks.  As shown in Table \ref{tab:conf}, the 8x8 probability of misclassification is an improvement on the 2x2 case, but far from being as good as the 32x32 case.  The 8x8 feature blocks might be used in an instance which there was some cost associated with feature block size, making the $32\time32 blocks too expensive to use.  In this type of situation, the $8\times8$ blocks might provide an acceptable compromise between cost and performance.
