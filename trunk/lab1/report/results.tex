section{Results}
Subsection{Cluster Generation}
1.	Visually, how does the unit contour relate to the cluster data?
The unit contour represents a collection of equally likely points in space. The elliptical unit standard deviation contours match the rough elliptical shape of the data clusters. The unit standard deviation contour does not enclose all data points.  This is completely expected as the random data points that make up the clusters were generated on a normal distribution with a given mean and covariance; we would not expect all data points to be within one standard deviation of the mean.  

subsection{Classification Boundaries}

2.	Comment on the classification boundaries. How do the different boundaries compare?

Parametric:

MED does not take the shape of the cluster into account. In the first case where two clusters are both non-circle, it gives a worse classifier than GED.

b.	When two classes have both the same probability and Gaussian distribution, as in the first case, GED is exactly the same as MAP. However, in the second case where the three classes are not equally likely, MAP prefers the class 1) is more compact, and 2) has a higher probability density in a given region. 

This can be shown from the following observation: the MAP boundary is closer to Class D than the GED boundary because it has a higher class probability (Nd=200) compared with Class C (Nc=100) and E (Ne=150). For the same reason, compared with the GED boundary, the MAP boundary between class C and E is closer to Class E.

Non-parametric:

a.	Since it is more sensitive to outliers, NN has more complex boundaries than 5-NN in two ways. Firstly, NN has more boundaries. Secondly, these boundaries are much less smooth than 5-NN.
b.	However, NN both runs faster and requires less memory since it does not demand the distance from the testing points to all the training data to be sorted.

Of the non-parametric classifiers, the 5NN classifier displayed a much simpler boundary than the NN classifier.  The key distinction is the fact that the 5NN, by ignoring the four nearest points, is making itself much less sensitive to outliers.  To explore the effect of the choice of k on the probability of error, we created a simple for loop to calculate the probability of error for the full range of k (ie 0<k<(number of points in smallest class).  This generally showed a sharp decrease initially, as outliers are ignored.  After the initial drop, the graphs behave slightly differently for the 2- and 3-class cases.  For the 2-class case, we found that the probability of error remains fairly flat for the majority of the values of k, with a spike as k approaches its maximum value.  For the 3-class case, however, it seems that the probability of error begins to climb again immediately following the initial drop.  This suggests that class structure is eroded more quickly due to the exclusion of good data for the 3-class case.  From this result, we might suspect that this trend would hold as the number of classes increases.

3.	Compare the results. Which error is smallest? What do you observe in the confusion matrices for CASE2?

In both of the two cases, MAP has the smallest error among the parametric classifiers. Besides, if the two classes are both Gaussian distributed and they have the same probability and same determinant of the covariance, MAP is the as good as GED. Furthermore, if the two classes have the same covariance and the line connecting the two means is the same as either the major or minor axis of the two classes, MED, GED and MAP are all the same.

For the non-parametric case, K-NN has smaller error than NN. This is again attributed to the fact that the former is less sensitive to outliers in the training data. It is observed from the confusion matrices that the elements in (1, 2) and (2, 1) are much smaller than the rest. This is caused by the fact that Class C and D has very little overlap with each other.

In the first case, MAP does better than K-NN. However, it is the other way around for the second case. Therefore, the relative performances of parametric and non-parametric classifiers are case-based even when the assumption for the shape of the class holds.

