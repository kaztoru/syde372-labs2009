The implementation for Lab 1 is done using MATLAB's class structures tomaximize the reusability and allow for experiementation beyond the requirementsof the lab. This is discussed further in Chapter \ref{cha:results}.\section{Properties and class functions}MATLAB classes were created for parametric and non-parametric classifiers. Theclasses, {\tt ParametricClass} and {\tt NonParametricClass} are presented inAppendix \ref{cha:code}.\subsection{Properties}The two classes store properties related to the pattern recognition problemsthey represent. The {\tt ParametricClass} stores for a class $A$ the values of$\mu_A$, $\Sigma_A$ and $p(A)$. The {\tt NonParametricClass} stores a clusterof $n$ points in a Gaussian distribution with the parameters $\mu$and $\Sigma$.\subsection{Class functions}Each class provides methods for calculating the various distance measuresassociated with the type of problem that it represents. The {\ttParametricClass} has functions for calculating $d^2$ using both MED and GED aswell as a function for calculating the value of $p(A) \cdot P(x|A)$ as ameasure of probability for MAP classification. The {\tt NonParametricClass} contains afunction for calculating distance to the class using kNN.\subsection{Calculations}Distance-squared by MED is calculated in the {\tt ParametricClass} class in the{\tt MED( point )} function. For a {\tt ParametricClass} $A$ and a point $p$,\begin{equation}d_{MED}^2 = (p - \mu_A)^T \cdot (p - \mu_A)\end{equation}Distance-squared by GED is calculated in the {\tt ParametricClass} class in the{\tt GED( point )} function. For a {\tt ParametricClass} $A$ and a point $p$,\begin{equation}d_{GED}^2 = (p - \mu_A)^T \cdot \Sigma_A^{-1} \cdot (p - \mu_A)\end{equation}The {\tt MAP( point )} function does not really calculate distance at all. Thevalue returned is one side of the Bayes Theorem inequality $\bar x \in A \iff p(\barx|A) \cdot P(A)$ where $A$ is the class calling the function. Equation 4.16 ofthe course notes gives one side of the inequality as\begin{align*}&P(A) \cdot \frac{1}{(2\pi)^{n/2}} \cdot \exp(-\frac{1}{2}(p - \mu_A)^T \cdot \Sigma_A^{-1} \cdot (p- \mu_A)) \\= &P(A) \cdot \frac{1}{(2 \pi)^{n/2}} \cdot \exp(-\frac{1}{2} \cdot d_{GED}^2)\end{align*}Since the $\frac{1}{(2 \pi)^{n/2}}$ portion of the equation is constant, it canbe removed from the comparison, giving the final weighted probability as\begin{equation}p_{weighted} = P(A) \cdot e^{-\frac{1}{2} \cdot d_{GED}^2}
\end{equation}Finally, kNN is calculated in {\tt NonParametricClass} in the {\tt kNN( point,k )} function. The point $p$ is used to generate an $2 \times n$ matrix$A$ where $A_{1j}$ is the x-coordinate and $A_{2j}$ is the y-coordinate of $p$.An $n \times n$ matrix with the distance-squared from each point in the class$C$ is computed by the function$$D = (A^T-C) \cdot (A^T - C)^T$$The diagonal entries of $D$ are converted to a vector, rooted and sorted. The$k$th element is then returned as the distance.\subsection{Plotting functions}The classes also provide helper functions for creating graphicalrepresentations of their data. {\tt ParametricClass} has a function forplotting a the unit standard deviation curve and {\tt NonParametricClass}contains a function for plotting the cluster of points the comprise the class.\section{Static methods}\subsection{Classification}The classes include methods for classifying points based on the variousdistance and probability methods. With the exception of the MAP classifier,their functionality is similar. The logic is defined in Algorithm\ref{ag:class}.\begin{algorithm}\caption{Classify a point based on distance to the classes}\label{ag:class}\begin{algorithmic}\STATE class number = 0\STATE minimum distance = $\infty$\FOR{ $i$ = 1 to $n_{classes}$ }\IF{distance to class $i \leq$ minimum distance}\STATE class number = $i$\STATE minimum distance = distance to class $i$\ENDIF\ENDFOR
\end{algorithmic}
\end{algorithm}The difference between the function for each classification method is thedistance function that is called to determine the distance from the point tothe class. In the MAP class is that the search is for the highest weightedprobability instead of the shortest distance. Otherwise the MAP classification algorithm is similar to the rest.\subsection{Class boundaries}Another static method included in the classes is a function to find the classboundaries using the different distance and probability methods. Thefunctions classify an $n \times m$ set of points in the x-y plane to generatethe class boundaries. The logic for these functions is defined in Algorithm\ref{ag:bounds}.\begin{algorithm}\caption{Populate the matrix that determines class boundaries}\label{ag:bounds}\begin{algorithmic}\STATE $C$ = an $n \times m$ matrix\FOR{$i$ = 1 to $n$}	\FOR{$j$ = 1 to $m$}	$C_{ij}$ = class of the point($x_i$, $y_J$)	\ENDFOR\ENDFOR
\end{algorithmic}\end{algorithm}The function returns an $n \times m$ matrix $C$ with the elements $C_{ij} =\{ 1,2 \ldots n_{classes} \}$. The contours of this are plotted on a graph toreveal the boundaries of the classes.\subsection{Class testing}Finally, the MATLAB classes provide two functions for testing; one fordetermining the confusion matrix and the other for calculating the probabilityof error ($P(\varepsilon)$) given a confusion matrix. The confusion matrixcalculators generate an $n \times n$ matrix with $n$ being the number ofclasses in the space. Using classes $C_i$ and test data $T_i$ with $i \in\{1,2, \ldots, n\}$, the method is defined in Algorithm \ref{ag:confusion}.\begin{algorithm}\caption{Calculate the confusion matrix for a given set of classes and testdata}\label{ag:confusion}\begin{algorithmic}\STATE $M_{confusion_{n,n}} =\begin{bmatrix}0 & \cdots & 0 \\  \vdots & \ddots & \vdots \\0 & \cdots & 0 \\\end{bmatrix}$\FOR{$i$ = 1 to $n_{test\_classes}$}\FOR{$j$ = 1 to $n_{points_{i}}$}\STATE class = the evaluated class of point $j$\STATE add 1 to $M_{confusion}$ at the cell (class, $i$)\ENDFOR\ENDFOR
\end{algorithmic}\end{algorithm}Algorithm \ref{ag:confusion} returns the confusion matrix, $M_{confusion}$. The confusionmatrix is then used to calculate $P(\varepsilon)$ as defined in Algorithm\ref{ag:p_err}.\begin{algorithm}\caption{Calculate the probability of error from a confusion matrix}\label{ag:p_err}\begin{algorithmic}\STATE correct assignments = diag($M_{confusion}$)\STATE incorrect assignments = $M_{confusion}$ - correct assignments\STATE $P(\varepsilon)$ = $\frac{\sum \text{elements of incorrect
assignments}}{\sum \text{elements of } M_{confusion}}$
\end{algorithmic}\end{algorithm}